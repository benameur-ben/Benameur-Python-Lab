"""
Benameur Python Lab - Professional Series
Distributed Asynchronous Web Crawler
--------------------------------------
Author: Benameur Mohamed
Entity: Benameur Soft
"""

import asyncio
import aiohttp
from bs4 import BeautifulSoup
import time
import logging

# Configure Logging / Ø¥Ø¹Ø¯Ø§Ø¯ Ø³Ø¬Ù„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BenameurCrawler:
    """
    A professional-grade asynchronous crawler capable of handling large-scale tasks.
    Ù…Ø³ØªÙƒØ´Ù ÙˆÙŠØ¨ Ù…ØªØ·ÙˆØ± ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù† Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ù‡Ø§Ù… ÙˆØ§Ø³Ø¹Ø© Ø§Ù„Ù†Ø·Ø§Ù‚.
    """
    
    def __init__(self, base_urls, concurrent_limit=5):
        self.base_urls = base_urls
        self.limit = concurrent_limit
        self.results = []
        self.semaphore = asyncio.Semaphore(concurrent_limit)

    async def fetch_page(self, session, url):
        """Fetches content from a URL with concurrency control / Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ø¹ Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„ØªØ²Ø§Ù…Ù†"""
        async with self.semaphore:
            try:
                logger.info(f"ğŸš€ Fetching: {url}")
                async with session.get(url, timeout=10) as response:
                    if response.status == 200:
                        content = await response.text()
                        return await self.parse(url, content)
                    else:
                        logger.warning(f"âš ï¸ Failed {url} with status {response.status}")
            except Exception as e:
                logger.error(f"âŒ Error at {url}: {str(e)}")
        return None

    async def parse(self, url, html):
        """Extracts metadata professionally / Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø§Ø­ØªØ±Ø§ÙÙŠØ©"""
        soup = BeautifulSoup(html, 'html.parser')
        title = soup.title.string if soup.title else "No Title"
        links = [a.get('href') for a in soup.find_all('a', href=True)][:5] # First 5 links
        data = {
            "url": url,
            "title": title.strip(),
            "links_count": len(links),
            "timestamp": time.time()
        }
        return data

    async def run(self):
        """Main execution flow / ØªØ¯ÙÙ‚ Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
        async with aiohttp.ClientSession() as session:
            tasks = [self.fetch_page(session, url) for url in self.base_urls]
            self.results = await asyncio.gather(*tasks)
            logger.info(f"âœ… Finished crawling {len(self.results)} targets.")
            return [r for r in self.results if r]

if __name__ == "__main__":
    # Example targets / Ø£Ù‡Ø¯Ø§Ù ØªØ¬Ø±ÙŠØ¨ÙŠØ©
    targets = [
        "https://www.google.com",
        "https://www.github.com",
        "https://www.python.org",
        "https://www.wikipedia.org"
    ]
    
    crawler = BenameurCrawler(targets)
    start_time = time.perf_counter()
    
    # Run the event loop / ØªØ´ØºÙŠÙ„ Ø­Ù„Ù‚Ø© Ø§Ù„Ø£Ø­Ø¯Ø§Ø«
    results = asyncio.run(crawler.run())
    
    end_time = time.perf_counter()
    print(f"\n--- Crawling Report (Benameur Soft) ---")
    for r in results:
        print(f"ğŸ“ {r['url']} | ğŸ·ï¸ {r['title']}")
    print(f"\nâ±ï¸ Total Time: {end_time - start_time:.2f} seconds")
